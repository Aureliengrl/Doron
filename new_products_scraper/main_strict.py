#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de Scraping STRICT des Produits pour DOR√ïN
Version 2.0 - ULTRA STRICT
- Upload SEULEMENT les produits 100% complets
- Supprime les produits si donn√©es manquantes
- 3 tentatives de scraping par produit
"""

import csv
import time
import random
import re
from datetime import datetime
from urllib.parse import urlparse
import firebase_admin
from firebase_admin import credentials, firestore
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# ============================================
# CONFIGURATION
# ============================================

FIREBASE_PROJECT_ID = "doron-b3011"
CSV_FILE = "links.csv"
LOG_FILE = "scraping_strict_log.txt"

# Nombre de tentatives de scraping par produit
MAX_RETRIES = 3

# ============================================
# INITIALISATION FIREBASE
# ============================================

def init_firebase():
    """Initialise Firebase Admin SDK"""
    try:
        cred = credentials.Certificate('serviceAccountKey.json')
        firebase_admin.initialize_app(cred)
        db = firestore.client()
        print("‚úÖ Firebase initialis√© avec succ√®s!")
        return db
    except Exception as e:
        print(f"‚ùå Erreur initialisation Firebase: {e}")
        print("\n‚ö†Ô∏è IMPORTANT: Assurez-vous que serviceAccountKey.json est pr√©sent!")
        return None

# ============================================
# INITIALISATION SELENIUM
# ============================================

def init_selenium():
    """Initialise Selenium avec Chrome headless"""
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

        driver = webdriver.Chrome(options=chrome_options)
        print("‚úÖ Selenium initialis√© avec succ√®s!")
        return driver
    except Exception as e:
        print(f"‚ùå Erreur initialisation Selenium: {e}")
        return None

# ============================================
# EXTRACTION DE MARQUE
# ============================================

def extract_brand_from_url(url):
    """Extrait la marque depuis l'URL - STRICT"""
    domain = urlparse(url).netloc.lower()

    brand_mapping = {
        'goldengoose': 'Golden Goose',
        'zara': 'Zara',
        'maje': 'Maje',
        'miumiu': 'Miu Miu',
        'rhode': 'Rhode',
        'sephora': 'Sephora',
        'lululemon': 'Lululemon',
        'messika': 'Messika',
        'backmarket': 'Back Market',
        'boulanger': 'Boulanger',
        'fnac': 'Fnac',
        'galerieslafayette': 'Galeries Lafayette',
        'ikea': 'Ikea',
        'maisonmargiela': 'Maison Margiela',
        'aloyoga': 'Alo Yoga',
        'zagbijoux': 'Zag Bijoux',
        'printemps': 'Printemps',
        'moonnude': 'Moon Nude',
        'rimowa': 'Rimowa',
    }

    for key, brand in brand_mapping.items():
        if key in domain:
            return brand

    # Si la marque n'est pas reconnue, retourner None pour forcer l'√©chec
    return None

# ============================================
# EXTRACTION DE DONN√âES - ULTRA PR√âCISE
# ============================================

def extract_product_data_strict(driver, url, attempt=1):
    """Extrait les donn√©es du produit avec s√©lecteurs ultra-pr√©cis par site"""
    try:
        print(f"      üîç Tentative {attempt}/{MAX_RETRIES}")

        # Charger la page
        driver.get(url)
        time.sleep(random.uniform(4, 7))  # D√©lai anti-blocage

        # Parser le HTML
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        domain = urlparse(url).netloc.lower()

        # ========== EXTRACTION DU NOM ==========
        name = None
        name_selectors = []

        # S√©lecteurs sp√©cifiques par site (prioritaires)
        if 'messika' in domain:
            name_selectors = [
                ('h1', {'class': 'product-name'}),
                ('h1', {'class': 'title'}),
                ('meta', {'property': 'og:title'}),
            ]
        elif 'backmarket' in domain:
            name_selectors = [
                ('h1', {'data-qa': 'product-title'}),
                ('h1', {'class': re.compile(r'title', re.I)}),
                ('meta', {'property': 'og:title'}),
            ]
        elif 'boulanger' in domain:
            name_selectors = [
                ('h1', {'class': 'product-title'}),
                ('h1', {'itemprop': 'name'}),
                ('meta', {'property': 'og:title'}),
            ]
        elif 'fnac' in domain:
            name_selectors = [
                ('h1', {'class': 'f-productHeader-Title'}),
                ('h1', {'data-testid': 'product-title'}),
                ('meta', {'property': 'og:title'}),
            ]
        elif 'galerieslafayette' in domain or 'printemps' in domain:
            name_selectors = [
                ('h1', {'class': 'ProductName'}),
                ('h1', {'class': re.compile(r'product.*name', re.I)}),
                ('meta', {'property': 'og:title'}),
            ]
        elif 'ikea' in domain:
            name_selectors = [
                ('h1', {'class': 'pip-header-section__title'}),
                ('span', {'class': 'pip-header-section__title--big'}),
                ('meta', {'property': 'og:title'}),
            ]
        elif 'maisonmargiela' in domain:
            name_selectors = [
                ('h1', {'class': 'product-name'}),
                ('span', {'class': 'base'}),
                ('meta', {'property': 'og:title'}),
            ]
        elif 'aloyoga' in domain:
            name_selectors = [
                ('h1', {'class': 'product-name'}),
                ('h1', {'data-testid': 'product-title'}),
                ('meta', {'property': 'og:title'}),
            ]
        elif 'zagbijoux' in domain:
            name_selectors = [
                ('h1', {'class': 'product-title'}),
                ('h1', {'itemprop': 'name'}),
                ('meta', {'property': 'og:title'}),
            ]
        elif 'moonnude' in domain:
            name_selectors = [
                ('h1', {'class': 'product-title'}),
                ('meta', {'property': 'og:title'}),
            ]
        elif 'rimowa' in domain:
            name_selectors = [
                ('h1', {'class': 'product-name'}),
                ('meta', {'property': 'og:title'}),
            ]
        else:
            # S√©lecteurs g√©n√©riques
            name_selectors = [
                ('meta', {'property': 'og:title'}),
                ('meta', {'name': 'twitter:title'}),
                ('h1', {'class': re.compile(r'product.*title|title.*product|product.*name', re.I)}),
                ('h1', {'data-testid': re.compile(r'product.*title|title', re.I)}),
                ('h1', {'itemprop': 'name'}),
                ('h1', {}),
            ]

        for tag, attrs in name_selectors:
            element = soup.find(tag, attrs)
            if element:
                if tag == 'meta':
                    name = element.get('content', '').strip()
                else:
                    name = element.get_text(strip=True)

                # Nettoyer le nom
                if name:
                    # Retirer les caract√®res sp√©ciaux en trop
                    name = re.sub(r'\s+', ' ', name)
                    name = name.strip()

                    # V√©rifier que le nom est valide (plus de 3 caract√®res)
                    if len(name) > 3:
                        break
                    else:
                        name = None

        # ========== EXTRACTION DU PRIX ==========
        price = None
        price_selectors = []

        if 'messika' in domain:
            price_selectors = [
                ('span', {'class': 'price'}),
                ('div', {'class': 'product-price'}),
                ('meta', {'property': 'product:price:amount'}),
            ]
        elif 'backmarket' in domain:
            price_selectors = [
                ('div', {'data-qa': 'price'}),
                ('span', {'class': re.compile(r'price', re.I)}),
                ('meta', {'property': 'og:price:amount'}),
            ]
        elif 'boulanger' in domain:
            price_selectors = [
                ('span', {'class': 'price'}),
                ('div', {'class': 'product-price'}),
                ('meta', {'property': 'product:price:amount'}),
            ]
        elif 'fnac' in domain:
            price_selectors = [
                ('div', {'class': 'f-priceBox-price'}),
                ('span', {'class': re.compile(r'price', re.I)}),
                ('meta', {'property': 'product:price:amount'}),
            ]
        elif 'galerieslafayette' in domain or 'printemps' in domain:
            price_selectors = [
                ('span', {'class': 'ProductPrice'}),
                ('div', {'class': re.compile(r'price', re.I)}),
                ('meta', {'property': 'product:price:amount'}),
            ]
        elif 'ikea' in domain:
            price_selectors = [
                ('span', {'class': 'pip-temp-price__integer'}),
                ('span', {'class': re.compile(r'price', re.I)}),
                ('meta', {'property': 'product:price:amount'}),
            ]
        else:
            price_selectors = [
                ('meta', {'property': 'og:price:amount'}),
                ('meta', {'property': 'product:price:amount'}),
                ('span', {'class': re.compile(r'price|prix', re.I)}),
                ('div', {'class': re.compile(r'price|prix', re.I)}),
                ('p', {'class': re.compile(r'price|prix', re.I)}),
                ('span', {'itemprop': 'price'}),
            ]

        for tag, attrs in price_selectors:
            elements = soup.find_all(tag, attrs) if tag != 'meta' else [soup.find(tag, attrs)]

            for element in elements:
                if not element:
                    continue

                if tag == 'meta':
                    text = element.get('content', '')
                else:
                    text = element.get_text(strip=True)

                # Chercher un pattern de prix robuste
                # Pattern avec centimes: 29.99, 29,99, 1 299,99, 1.299,99
                price_match = re.search(r'(\d{1,3}(?:[\s.,]?\d{3})*)[.,](\d{2})', text)
                if price_match:
                    price_str = price_match.group(0)
                    # Nettoyer: retirer espaces, remplacer virgule par point
                    price_str = price_str.replace(' ', '').replace(',', '.')
                    # Si format 1.299.99, garder seulement le dernier point
                    if price_str.count('.') > 1:
                        parts = price_str.split('.')
                        price_str = ''.join(parts[:-1]) + '.' + parts[-1]

                    try:
                        price = float(price_str)
                        if price > 0:
                            break
                    except:
                        continue

                # Pattern sans centimes
                if not price:
                    price_match = re.search(r'(\d{1,3}(?:[\s.,]?\d{3})*)', text)
                    if price_match:
                        price_str = price_match.group(1).replace(' ', '').replace(',', '')
                        try:
                            price = float(price_str)
                            if price > 0:
                                break
                        except:
                            continue

            if price and price > 0:
                break

        # ========== EXTRACTION DE L'IMAGE ==========
        image_url = None

        # M√©thode 1: Meta tags (prioritaire)
        meta_image_tags = [
            ('meta', {'property': 'og:image'}),
            ('meta', {'name': 'twitter:image'}),
            ('meta', {'itemprop': 'image'}),
        ]

        for tag, attrs in meta_image_tags:
            element = soup.find(tag, attrs)
            if element:
                img_candidate = element.get('content', '')
                if img_candidate and img_candidate.startswith('http'):
                    # V√©rifier que c'est une vraie image (pas un placeholder)
                    if not any(word in img_candidate.lower() for word in ['placeholder', 'default', 'noimage', 'blank']):
                        image_url = img_candidate
                        break

        # M√©thode 2: Images du produit principal
        if not image_url:
            img_selectors = [
                ('img', {'class': re.compile(r'product.*image|main.*image|hero.*image|gallery.*image', re.I)}),
                ('img', {'data-testid': re.compile(r'product.*image|main.*image', re.I)}),
                ('img', {'itemprop': 'image'}),
                ('img', {'id': re.compile(r'product.*image|main.*image', re.I)}),
                ('img', {'alt': re.compile(r'product|main', re.I)}),
            ]

            for tag, attrs in img_selectors:
                img = soup.find(tag, attrs)
                if img:
                    img_candidate = img.get('src') or img.get('data-src') or img.get('data-original') or img.get('data-lazy') or img.get('srcset')

                    if img_candidate:
                        # Si c'est un srcset, prendre la premi√®re URL
                        if 'srcset' in str(img_candidate):
                            img_candidate = img_candidate.split(',')[0].split(' ')[0]

                        # Nettoyer l'URL
                        if img_candidate.startswith('//'):
                            img_candidate = 'https:' + img_candidate
                        elif img_candidate.startswith('/'):
                            parsed = urlparse(url)
                            img_candidate = f"{parsed.scheme}://{parsed.netloc}{img_candidate}"

                        # V√©rifier que c'est une vraie image
                        if img_candidate.startswith('http') and not any(word in img_candidate.lower() for word in ['placeholder', 'default', 'noimage', 'blank']):
                            image_url = img_candidate
                            break

        # ========== EXTRACTION DE LA DESCRIPTION ==========
        description = None
        desc_selectors = [
            ('meta', {'property': 'og:description'}),
            ('meta', {'name': 'description'}),
            ('div', {'class': re.compile(r'description|product.*desc', re.I)}),
            ('p', {'class': re.compile(r'description', re.I)}),
        ]

        for tag, attrs in desc_selectors:
            element = soup.find(tag, attrs)
            if element:
                if tag == 'meta':
                    description = element.get('content', '').strip()
                else:
                    description = element.get_text(strip=True)

                if description and len(description) > 20:
                    description = description[:500]
                    break

        return {
            'name': name,
            'price': price,
            'image': image_url,
            'description': description
        }

    except Exception as e:
        print(f"      ‚ùå Erreur extraction (tentative {attempt}): {e}")
        return None

# ============================================
# VALIDATION STRICTE
# ============================================

def validate_product_strict(product_data, brand):
    """Valide qu'un produit a TOUS les champs obligatoires - MODE STRICT"""
    errors = []

    # Nom obligatoire (minimum 3 caract√®res)
    if not product_data.get('name') or len(str(product_data['name']).strip()) < 3:
        errors.append("Nom manquant ou trop court")

    # Marque obligatoire et reconnue
    if not brand or brand == 'Unknown':
        errors.append("Marque inconnue ou non reconnue")

    # Prix obligatoire (> 0)
    price = product_data.get('price')
    if not price or not isinstance(price, (int, float)) or price <= 0:
        errors.append("Prix manquant ou invalide")

    # Image obligatoire (URL valide)
    image = product_data.get('image')
    if not image or not str(image).startswith('http'):
        errors.append("Image manquante ou URL invalide")

    return len(errors) == 0, errors

# ============================================
# G√âN√âRATION DE TAGS & CAT√âGORIES (IDENTIQUE)
# ============================================

def generate_tags(product_data, brand):
    """G√©n√®re automatiquement les tags bas√©s sur les donn√©es du produit"""
    tags = []
    name = (product_data.get('name') or '').lower()
    description = (product_data.get('description') or '').lower()
    price = product_data.get('price') or 0

    full_text = f"{name} {description}"

    # Genre
    if any(word in full_text for word in ['femme', 'woman', 'women', 'her', 'elle', 'f√©minin', 'female', 'pour femme']):
        tags.append('femme')
    if any(word in full_text for word in ['homme', 'man', 'men', 'his', 'lui', 'masculin', 'male', 'pour homme']):
        tags.append('homme')
    if any(word in full_text for word in ['unisex', 'mixte', 'all', 'unisexe']):
        tags.extend(['femme', 'homme'])

    # Cat√©gories de produits
    if any(word in full_text for word in ['sneakers', 'baskets', 'running', 'trainer', 'chaussures', 'shoes', 'boots', 'bottes', 'mocassins']):
        tags.extend(['chaussures', 'mode'])
    if any(word in full_text for word in ['pull', 'sweater', 'sweat', 'veste', 'jacket', 'jean', 't-shirt', 'chemise', 'robe', 'dress', 'pantalon']):
        tags.extend(['vetements', 'mode'])
    if any(word in full_text for word in ['sac', 'bag', 'ceinture', 'belt', 'lunettes', 'sunglasses', 'bijoux', 'bracelet', 'collier', 'bague']):
        tags.extend(['accessoires'])
    if any(word in full_text for word in ['parfum', 'fragrance', 'maquillage', 'makeup', 'skincare', 'soin', 'beaute', 'beauty', 'creme', 'serum']):
        tags.extend(['beaute'])
    if any(word in full_text for word in ['yoga', 'sport', 'fitness', 'athletic', 'running', 'legging', 'brassiere']):
        tags.extend(['sport', 'fitness'])
    if any(word in full_text for word in ['iphone', 'macbook', 'ipad', 'samsung', 'console', 'playstation', 'laptop', 'ordinateur', 'casque', 'airpods', 'ecouteurs']):
        tags.extend(['tech', 'electronique'])
    if any(word in full_text for word in ['meuble', 'decoration', 'maison', 'home', 'vase', 'table', 'chaise']):
        tags.extend(['maison', 'decoration'])
    if any(word in full_text for word in ['valise', 'luggage', 'bagage', 'suitcase', 'travel', 'voyage']):
        tags.extend(['accessoires', 'voyage'])

    # Budget
    if price < 50:
        tags.extend(['budget_petit', 'abordable'])
    elif price < 150:
        tags.extend(['budget_moyen', 'accessible'])
    elif price < 400:
        tags.extend(['budget_luxe', 'premium'])
    else:
        tags.extend(['budget_premium', 'luxe'])

    # Style
    if any(word in full_text for word in ['luxe', 'luxury', 'premium', 'designer']):
        tags.append('luxe')
    if any(word in full_text for word in ['elegant', '√©l√©gant', 'chic']):
        tags.append('elegant')
    if any(word in full_text for word in ['casual', 'd√©contract√©']):
        tags.append('casual')
    if any(word in full_text for word in ['sportif', 'sport', 'athletic']):
        tags.append('sportif')
    if any(word in full_text for word in ['moderne', 'modern', 'tendance']):
        tags.append('moderne')

    # Tags sp√©cifiques par marque
    brand_tags = {
        'Messika': ['luxe', 'bijoux', 'diamant', 'joaillerie', 'premium', 'francais'],
        'Back Market': ['tech', 'reconditionne', 'ecologique', 'electronique', 'durable'],
        'Boulanger': ['tech', 'electronique', 'maison'],
        'Fnac': ['tech', 'culture', 'multimedia', 'electronique'],
        'Galeries Lafayette': ['luxe', 'mode', 'premium', 'parisien'],
        'Ikea': ['maison', 'decoration', 'design', 'scandinave', 'abordable'],
        'Maison Margiela': ['luxe', 'haute_couture', 'designer', 'avant-garde', 'premium'],
        'Alo Yoga': ['sport', 'yoga', 'wellness', 'fitness', 'athleisure'],
        'Zag Bijoux': ['bijoux', 'accessoires', 'tendance', 'francais'],
        'Printemps': ['mode', 'luxe', 'parisien'],
        'Moon Nude': ['beaute', 'accessoires', 'makeup'],
        'Rimowa': ['luxe', 'voyage', 'premium', 'allemand', 'design'],
        'Golden Goose': ['luxe', 'italien', 'sneakers', 'streetwear', 'designer'],
        'Zara': ['tendance', 'accessible', 'mode', 'fast-fashion'],
        'Sephora': ['beaute', 'maquillage', 'cosmetics'],
        'Lululemon': ['sportif', 'yoga', 'qualite', 'performance', 'fitness'],
        'Miu Miu': ['luxe', 'haute_couture', 'italien', 'designer', 'premium'],
        'Maje': ['mode', 'francais', 'elegant', 'contemporain'],
    }

    if brand in brand_tags:
        tags.extend(brand_tags[brand])

    # √Çge cible
    tags.append('20-30ans')
    if price > 300:
        tags.append('30-50ans')

    return list(set(tags))

def generate_categories(product_data, brand):
    """G√©n√®re les cat√©gories du produit"""
    categories = []
    name = (product_data.get('name') or '').lower()
    description = (product_data.get('description') or '').lower()
    full_text = f"{name} {description}"

    # Cat√©gories principales
    if any(word in full_text for word in ['sneakers', 'chaussures', 'shoes', 'boots', 'sandales', 'mocassins', 'baskets']):
        categories.append('chaussures')
    if any(word in full_text for word in ['sac', 'bag', 'purse', 'handbag', 'tote', 'pochette', 'ceinture', 'lunettes', 'bijoux', 'bracelet', 'collier', 'bague', 'valise', 'luggage']):
        categories.append('accessoires')
    if any(word in full_text for word in ['pull', 'sweater', 'sweat', 'hoodie', 'top', 'shirt', 'veste', 'jacket', 'jean', 'pantalon', 'robe', 'dress']):
        categories.append('vetements')
    if any(word in full_text for word in ['parfum', 'fragrance', 'perfume']):
        categories.append('parfums')
    if any(word in full_text for word in ['maquillage', 'makeup', 'lipstick', 'mascara', 'foundation', 'palette']):
        categories.append('maquillage')
    if any(word in full_text for word in ['skincare', 'soin', 'cream', 'serum', 'moisturizer']):
        categories.append('beaute')
    if any(word in full_text for word in ['sport', 'yoga', 'legging', 'brassiere', 'athletic', 'fitness']):
        categories.append('sport')
    if any(word in full_text for word in ['iphone', 'macbook', 'ipad', 'samsung', 'console', 'playstation', 'ordinateur', 'laptop', 'casque', 'airpods', 'tech', 'electronique']):
        categories.append('tech')
    if any(word in full_text for word in ['vase', 'table', 'cadre', 'decoration', 'maison', 'meuble']):
        categories.append('maison')

    # Fallback par marque
    if not categories:
        if brand in ['Sephora', 'Rhode', 'Moon Nude']:
            categories.append('beaute')
        elif brand in ['Lululemon', 'Alo Yoga']:
            categories.append('sport')
        elif brand in ['Messika', 'Zag Bijoux']:
            categories.append('accessoires')
        elif brand in ['Back Market', 'Boulanger', 'Fnac']:
            categories.append('tech')
        elif brand == 'Ikea':
            categories.append('maison')
        elif brand == 'Rimowa':
            categories.append('accessoires')
        else:
            categories.append('mode')

    return categories

# ============================================
# UPLOAD FIREBASE
# ============================================

def upload_to_firebase(db, product):
    """Upload un produit dans Firebase Firestore"""
    try:
        doc_ref = db.collection('gifts').document()
        doc_ref.set(product)
        print(f"      ‚úÖ Upload√© dans Firebase (ID: {doc_ref.id})")
        return True, doc_ref.id
    except Exception as e:
        print(f"      ‚ùå Erreur upload Firebase: {e}")
        return False, None

# ============================================
# FONCTION PRINCIPALE - MODE STRICT
# ============================================

def scrape_and_upload_strict():
    """Fonction principale de scraping - MODE ULTRA STRICT"""

    print("=" * 70)
    print("üï∑Ô∏è  SCRAPING STRICT DES PRODUITS DOR√ïN")
    print("   ‚ö†Ô∏è  MODE STRICT: Seuls les produits 100% complets seront upload√©s")
    print("=" * 70)
    print()

    # Initialiser Firebase
    db = init_firebase()
    if not db:
        print("‚ùå Impossible de continuer sans Firebase!")
        return

    # Initialiser Selenium
    driver = init_selenium()
    if not driver:
        print("‚ùå Impossible de continuer sans Selenium!")
        return

    # Lire les URLs depuis le CSV
    try:
        with open(CSV_FILE, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            urls = [row[0] for row in reader if row and row[0].startswith('http')]
    except Exception as e:
        print(f"‚ùå Erreur lecture CSV: {e}")
        return

    print(f"üìã {len(urls)} URLs √† scraper\n")

    # Ouvrir le fichier de log
    with open(LOG_FILE, 'w', encoding='utf-8') as log:
        log.write(f"{'='*70}\n")
        log.write(f"SCRAPING STRICT D√âMARR√â: {datetime.now()}\n")
        log.write(f"{'='*70}\n\n")

        success_count = 0
        fail_count = 0
        rejected_incomplete = 0

        # Scraper chaque URL
        for index, url in enumerate(urls, 1):
            print(f"\n[{index}/{len(urls)}] üîç Scraping: {url}")
            log.write(f"\n[{index}/{len(urls)}] URL: {url}\n")

            # Extraire la marque
            brand = extract_brand_from_url(url)
            if not brand:
                print(f"   ‚ùå MARQUE NON RECONNUE - SKIP")
                log.write(f"   ‚ùå REJET√â: Marque non reconnue\n")
                fail_count += 1
                continue

            print(f"   üè∑Ô∏è  Marque: {brand}")
            log.write(f"   Marque: {brand}\n")

            # Tenter le scraping (avec retry)
            product_data = None
            for attempt in range(1, MAX_RETRIES + 1):
                product_data = extract_product_data_strict(driver, url, attempt)

                if product_data:
                    # Valider le produit
                    is_valid, errors = validate_product_strict(product_data, brand)

                    if is_valid:
                        break  # Succ√®s!
                    else:
                        print(f"      ‚ö†Ô∏è  Produit incomplet: {', '.join(errors)}")
                        log.write(f"      ‚ö†Ô∏è Tentative {attempt} - Incomplet: {', '.join(errors)}\n")

                        if attempt < MAX_RETRIES:
                            print(f"      üîÑ Nouvelle tentative...")
                            time.sleep(random.uniform(3, 5))
                        else:
                            product_data = None  # √âchec apr√®s toutes les tentatives

                else:
                    if attempt < MAX_RETRIES:
                        print(f"      üîÑ Nouvelle tentative...")
                        time.sleep(random.uniform(3, 5))

            # V√©rifier le r√©sultat final
            if not product_data:
                print(f"   ‚ùå √âCHEC apr√®s {MAX_RETRIES} tentatives - SKIP")
                log.write(f"   ‚ùå REJET√â: √âchec apr√®s {MAX_RETRIES} tentatives\n")
                fail_count += 1
                continue

            # Validation finale
            is_valid, errors = validate_product_strict(product_data, brand)
            if not is_valid:
                print(f"   ‚ùå PRODUIT INCOMPLET - REJET√â")
                print(f"      Raisons: {', '.join(errors)}")
                log.write(f"   ‚ùå REJET√â: {', '.join(errors)}\n")
                rejected_incomplete += 1
                fail_count += 1
                continue

            # Produit valide - Afficher les donn√©es
            print(f"   ‚úÖ {product_data['name']}")
            print(f"   üí∞ Prix: {product_data['price']}‚Ç¨")
            print(f"   üñºÔ∏è  Image: {product_data['image'][:60]}...")
            log.write(f"   ‚úÖ Nom: {product_data['name']}\n")
            log.write(f"   üí∞ Prix: {product_data['price']}‚Ç¨\n")
            log.write(f"   üñºÔ∏è Image: {product_data['image']}\n")

            # G√©n√©rer tags et cat√©gories
            tags = generate_tags(product_data, brand)
            categories = generate_categories(product_data, brand)

            print(f"   üè∑Ô∏è  Tags ({len(tags)}): {', '.join(tags[:8])}...")
            print(f"   üìÇ Cat√©gories: {', '.join(categories)}")
            log.write(f"   üè∑Ô∏è Tags ({len(tags)}): {', '.join(tags)}\n")
            log.write(f"   üìÇ Cat√©gories: {', '.join(categories)}\n")

            # Cr√©er l'objet produit complet
            product = {
                'name': product_data['name'],
                'brand': brand,
                'price': product_data['price'],
                'url': url,
                'image': product_data['image'],
                'description': product_data.get('description') or f"Produit {brand} de qualit√© sup√©rieure.",
                'categories': categories,
                'tags': tags,
                'active': True,
                'source': 'strict_scraping',
                'created_at': firestore.SERVER_TIMESTAMP,
                'popularity': random.randint(60, 95),
                # Compatibilit√© ancien schema
                'product_photo': product_data['image'],
                'product_title': product_data['name'],
                'product_url': url,
                'product_price': str(product_data['price']),
            }

            # Upload dans Firebase
            success, doc_id = upload_to_firebase(db, product)
            if success:
                success_count += 1
                log.write(f"   ‚úÖ UPLOAD√â (ID: {doc_id})\n")
            else:
                fail_count += 1
                log.write(f"   ‚ùå √âCHEC UPLOAD\n")

            # Pause anti-blocage
            delay = random.uniform(4, 7)
            print(f"   ‚è≥ Pause {delay:.1f}s...")
            time.sleep(delay)

        # R√©sum√© final
        print("\n")
        print("=" * 70)
        print("üìä R√âSULTATS FINAUX (MODE STRICT):")
        print(f"   ‚úÖ Produits scrap√©s et upload√©s: {success_count}")
        print(f"   ‚ùå √âchecs totaux: {fail_count}")
        print(f"   üö´ Produits rejet√©s (incomplets): {rejected_incomplete}")
        if success_count + fail_count > 0:
            print(f"   üìà Taux de r√©ussite: {(success_count/(success_count+fail_count)*100):.1f}%")
        print("=" * 70)

        log.write(f"\n\n{'='*70}\n")
        log.write(f"R√âSUM√â:\n")
        log.write(f"   ‚úÖ Succ√®s: {success_count}\n")
        log.write(f"   ‚ùå √âchecs: {fail_count}\n")
        log.write(f"   üö´ Rejet√©s (incomplets): {rejected_incomplete}\n")
        if success_count + fail_count > 0:
            log.write(f"   üìà Taux: {(success_count/(success_count+fail_count)*100):.1f}%\n")
        log.write(f"{'='*70}\n")

    # Fermer le navigateur
    driver.quit()
    print("\nüéâ SCRAPING TERMIN√â!")
    print(f"üìù Logs sauvegard√©s dans: {LOG_FILE}")
    print(f"\nüí° Conseil: Utilisez cleanup_firebase.py pour nettoyer les produits incomplets existants")

# ============================================
# POINT D'ENTR√âE
# ============================================

if __name__ == "__main__":
    scrape_and_upload_strict()
