#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de Scraping R√âEL des Produits pour DOR√ïN
√Ä ex√©cuter sur Replit.com
"""

import csv
import time
import random
import re
from datetime import datetime
from urllib.parse import urlparse
import firebase_admin
from firebase_admin import credentials, firestore
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# ============================================
# CONFIGURATION
# ============================================

FIREBASE_PROJECT_ID = "doron-b3011"
CSV_FILE = "links.csv"
LOG_FILE = "scraping_log.txt"

# ============================================
# INITIALISATION FIREBASE
# ============================================

def init_firebase():
    """Initialise Firebase Admin SDK"""
    try:
        cred = credentials.Certificate('serviceAccountKey.json')
        firebase_admin.initialize_app(cred)
        db = firestore.client()
        print("‚úÖ Firebase initialis√© avec succ√®s!")
        return db
    except Exception as e:
        print(f"‚ùå Erreur initialisation Firebase: {e}")
        print("\n‚ö†Ô∏è IMPORTANT: Assurez-vous que serviceAccountKey.json est pr√©sent!")
        return None

# ============================================
# INITIALISATION SELENIUM
# ============================================

def init_selenium():
    """Initialise Selenium avec Chrome headless"""
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

        driver = webdriver.Chrome(options=chrome_options)
        print("‚úÖ Selenium initialis√© avec succ√®s!")
        return driver
    except Exception as e:
        print(f"‚ùå Erreur initialisation Selenium: {e}")
        return None

# ============================================
# EXTRACTION DE DONN√âES
# ============================================

def extract_brand_from_url(url):
    """Extrait la marque depuis l'URL"""
    domain = urlparse(url).netloc.lower()

    if 'goldengoose' in domain:
        return 'Golden Goose'
    elif 'zara' in domain:
        return 'Zara'
    elif 'maje' in domain:
        return 'Maje'
    elif 'miumiu' in domain:
        return 'Miu Miu'
    elif 'rhode' in domain:
        return 'Rhode'
    elif 'sephora' in domain:
        return 'Sephora'
    elif 'lululemon' in domain:
        return 'Lululemon'
    else:
        return 'Unknown'

def extract_product_data(driver, url):
    """Extrait les donn√©es du produit depuis la page web"""
    try:
        # Charger la page
        driver.get(url)
        time.sleep(random.uniform(2, 4))  # D√©lai anti-blocage

        # Parser le HTML
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # ========== EXTRACTION DU NOM ==========
        name = None
        name_selectors = [
            ('h1', {'class': re.compile(r'product.*title|title.*product', re.I)}),
            ('h1', {'data-testid': 'product-title'}),
            ('h1', {}),
            ('span', {'class': re.compile(r'product.*name|name.*product', re.I)}),
            ('div', {'class': re.compile(r'product.*title', re.I)}),
        ]

        for tag, attrs in name_selectors:
            element = soup.find(tag, attrs)
            if element and element.get_text(strip=True):
                name = element.get_text(strip=True)
                break

        if not name:
            # Fallback: chercher dans meta tags
            meta_title = soup.find('meta', {'property': 'og:title'})
            if meta_title:
                name = meta_title.get('content', '').strip()

        # ========== EXTRACTION DU PRIX ==========
        price = None
        price_selectors = [
            ('span', {'class': re.compile(r'price|prix', re.I)}),
            ('div', {'class': re.compile(r'price|prix', re.I)}),
            ('p', {'class': re.compile(r'price|prix', re.I)}),
            ('span', {'data-testid': 'price'}),
        ]

        for tag, attrs in price_selectors:
            elements = soup.find_all(tag, attrs)
            for element in elements:
                text = element.get_text(strip=True)
                # Chercher un pattern de prix (ex: 29.99, 29,99, ‚Ç¨29.99, $29.99)
                price_match = re.search(r'(\d+[.,]\d{2}|\d+)', text)
                if price_match:
                    price_str = price_match.group(1).replace(',', '.')
                    try:
                        price = float(price_str)
                        break
                    except:
                        continue
            if price:
                break

        # ========== EXTRACTION DE L'IMAGE ==========
        image_url = None

        # M√©thode 1: Chercher dans meta tags Open Graph
        og_image = soup.find('meta', {'property': 'og:image'})
        if og_image:
            image_url = og_image.get('content', '')

        # M√©thode 2: Chercher les images principales du produit
        if not image_url:
            image_selectors = [
                ('img', {'class': re.compile(r'product.*image|main.*image|hero.*image', re.I)}),
                ('img', {'data-testid': 'product-image'}),
                ('img', {'itemprop': 'image'}),
            ]

            for tag, attrs in image_selectors:
                img = soup.find(tag, attrs)
                if img:
                    image_url = img.get('src') or img.get('data-src') or img.get('data-original')
                    if image_url:
                        break

        # Nettoyer l'URL de l'image
        if image_url:
            if image_url.startswith('//'):
                image_url = 'https:' + image_url
            elif image_url.startswith('/'):
                parsed = urlparse(url)
                image_url = f"{parsed.scheme}://{parsed.netloc}{image_url}"

        # ========== EXTRACTION DE LA DESCRIPTION ==========
        description = None
        desc_selectors = [
            ('div', {'class': re.compile(r'description|product.*desc', re.I)}),
            ('p', {'class': re.compile(r'description', re.I)}),
            ('meta', {'name': 'description'}),
            ('meta', {'property': 'og:description'}),
        ]

        for tag, attrs in desc_selectors:
            element = soup.find(tag, attrs)
            if element:
                if tag == 'meta':
                    description = element.get('content', '').strip()
                else:
                    description = element.get_text(strip=True)

                if description and len(description) > 20:
                    description = description[:500]  # Limiter √† 500 caract√®res
                    break

        return {
            'name': name,
            'price': price,
            'image': image_url,
            'description': description
        }

    except Exception as e:
        print(f"    ‚ùå Erreur extraction: {e}")
        return None

# ============================================
# G√âN√âRATION DE TAGS
# ============================================

def generate_tags(product_data, brand):
    """G√©n√®re automatiquement les tags bas√©s sur les donn√©es du produit"""
    tags = []
    name = (product_data.get('name') or '').lower()
    description = (product_data.get('description') or '').lower()
    price = product_data.get('price') or 0

    full_text = f"{name} {description}"

    # GENRE
    if any(word in full_text for word in ['femme', 'woman', 'women', 'her', 'elle', 'f√©minin']):
        tags.append('femme')
    if any(word in full_text for word in ['homme', 'man', 'men', 'his', 'lui', 'masculin']):
        tags.append('homme')
    if any(word in full_text for word in ['unisex', 'mixte', 'all']):
        tags.extend(['femme', 'homme'])

    # √ÇGE
    if any(word in full_text for word in ['enfant', 'kid', 'child', 'junior', 'b√©b√©', 'baby']):
        tags.append('enfant')
    else:
        tags.append('adulte')

    # BUDGET
    if price < 50:
        tags.append('budget_petit')
    elif price < 150:
        tags.append('budget_moyen')
    elif price < 400:
        tags.append('budget_luxe')
    else:
        tags.append('budget_premium')

    # STYLE
    if any(word in full_text for word in ['sport', 'athletic', 'running', 'yoga', 'fitness', 'gym']):
        tags.append('sportif')
    if any(word in full_text for word in ['casual', 'd√©contract√©', 'relaxed', 'everyday']):
        tags.append('casual')
    if any(word in full_text for word in ['elegant', '√©l√©gant', 'chic', 'formal', 'luxe', 'luxury']):
        tags.append('elegant')
    if any(word in full_text for word in ['vintage', 'retro', 'classic', 'classique']):
        tags.append('vintage')
    if any(word in full_text for word in ['modern', 'moderne', 'contemporary', 'trend']):
        tags.append('moderne')

    # OCCASIONS
    if any(word in full_text for word in ['travail', 'work', 'office', 'business', 'professional']):
        tags.append('travail')
    if any(word in full_text for word in ['soir√©e', 'party', 'evening', 'cocktail', 'gala']):
        tags.append('soiree')
    if any(word in full_text for word in ['quotidien', 'daily', 'everyday', 'casual']):
        tags.append('quotidien')

    # MARQUE SP√âCIFIQUE
    if brand == 'Golden Goose':
        tags.extend(['luxe', 'italien', 'sneakers'])
    elif brand == 'Zara':
        tags.extend(['tendance', 'accessible'])
    elif brand == 'Sephora' or brand == 'Rhode':
        tags.extend(['beaute', 'skincare'])
    elif brand == 'Lululemon':
        tags.extend(['sportif', 'yoga', 'qualite'])
    elif brand == 'Miu Miu':
        tags.extend(['luxe', 'haute_couture', 'italien'])

    return list(set(tags))  # √âliminer les doublons

def generate_categories(product_data, brand):
    """G√©n√®re les cat√©gories du produit"""
    categories = []
    name = (product_data.get('name') or '').lower()
    description = (product_data.get('description') or '').lower()

    full_text = f"{name} {description}"

    # CAT√âGORIES PRINCIPALES
    if any(word in full_text for word in ['sneakers', 'chaussures', 'shoes', 'boots', 'sandales']):
        categories.append('chaussures')
    if any(word in full_text for word in ['sac', 'bag', 'purse', 'handbag', 'tote']):
        categories.append('accessoires')
    if any(word in full_text for word in ['pull', 'sweater', 'sweat', 'hoodie', 'cardigan', 'top', 'shirt', 'chemise']):
        categories.append('vetements')
    if any(word in full_text for word in ['parfum', 'fragrance', 'perfume', 'eau de toilette']):
        categories.append('parfums')
    if any(word in full_text for word in ['maquillage', 'makeup', 'lipstick', 'mascara', 'foundation']):
        categories.append('maquillage')
    if any(word in full_text for word in ['skincare', 'soin', 'cream', 'serum', 'moisturizer']):
        categories.append('beaute')
    if any(word in full_text for word in ['sport', 'yoga', 'legging', 'brassiere', 'athletic']):
        categories.append('sport')

    # Cat√©gories bas√©es sur la marque
    if brand in ['Sephora', 'Rhode']:
        if 'beaute' not in categories and 'maquillage' not in categories:
            categories.append('beaute')
    elif brand == 'Lululemon':
        if 'sport' not in categories:
            categories.append('sport')
    elif brand in ['Golden Goose', 'Zara', 'Maje', 'Miu Miu']:
        if 'mode' not in categories:
            categories.append('mode')

    return categories if categories else ['mode']

# ============================================
# UPLOAD FIREBASE
# ============================================

def upload_to_firebase(db, product):
    """Upload un produit dans Firebase Firestore"""
    try:
        doc_ref = db.collection('gifts').document()
        doc_ref.set(product)
        print(f"    ‚úÖ Upload√© dans Firebase (ID: {doc_ref.id})")
        return True
    except Exception as e:
        print(f"    ‚ùå Erreur upload Firebase: {e}")
        return False

# ============================================
# FONCTION PRINCIPALE
# ============================================

def scrape_and_upload():
    """Fonction principale de scraping et upload"""

    print("=" * 60)
    print("üï∑Ô∏è  SCRAPING R√âEL DES PRODUITS DOR√ïN")
    print("=" * 60)
    print()

    # Initialiser Firebase
    db = init_firebase()
    if not db:
        print("‚ùå Impossible de continuer sans Firebase!")
        return

    # Initialiser Selenium
    driver = init_selenium()
    if not driver:
        print("‚ùå Impossible de continuer sans Selenium!")
        return

    # Lire les URLs depuis le CSV
    try:
        with open(CSV_FILE, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            urls = [row[0] for row in reader if row and row[0].startswith('http')]
    except Exception as e:
        print(f"‚ùå Erreur lecture CSV: {e}")
        return

    print(f"üìã {len(urls)} URLs √† scraper\n")

    # Ouvrir le fichier de log
    with open(LOG_FILE, 'a', encoding='utf-8') as log:
        log.write(f"\n\n{'='*60}\n")
        log.write(f"SCRAPING D√âMARR√â: {datetime.now()}\n")
        log.write(f"{'='*60}\n\n")

        success_count = 0
        fail_count = 0

        # Scraper chaque URL
        for index, url in enumerate(urls, 1):
            print(f"\n[{index}/{len(urls)}] üîç Scraping: {url}")
            log.write(f"\n[{index}/{len(urls)}] URL: {url}\n")

            # Extraire la marque
            brand = extract_brand_from_url(url)
            print(f"    üè∑Ô∏è  Marque: {brand}")

            # Scraper les donn√©es
            product_data = extract_product_data(driver, url)

            if not product_data:
                print(f"    ‚ùå √âchec extraction")
                log.write(f"    ‚ùå √âCHEC: Impossible d'extraire les donn√©es\n")
                fail_count += 1
                continue

            # V√©rifier que les donn√©es essentielles sont pr√©sentes
            if not product_data.get('name'):
                print(f"    ‚ö†Ô∏è  Nom manquant - SKIP")
                log.write(f"    ‚ùå √âCHEC: Nom manquant\n")
                fail_count += 1
                continue

            print(f"    ‚úÖ {product_data['name']}")

            if product_data.get('price'):
                print(f"    üí∞ Prix: {product_data['price']}‚Ç¨")
            else:
                print(f"    ‚ö†Ô∏è  Prix non trouv√©")

            if product_data.get('image'):
                print(f"    üñºÔ∏è  Image: OK")
            else:
                print(f"    ‚ö†Ô∏è  Image non trouv√©e")

            # G√©n√©rer tags et cat√©gories
            tags = generate_tags(product_data, brand)
            categories = generate_categories(product_data, brand)

            print(f"    üè∑Ô∏è  Tags: {', '.join(tags[:5])}...")
            print(f"    üìÇ Cat√©gories: {', '.join(categories)}")

            # Cr√©er l'objet produit complet
            product = {
                'name': product_data['name'],
                'brand': brand,
                'price': product_data.get('price') or 0,
                'url': url,
                'image': product_data.get('image') or '',
                'description': product_data.get('description') or f"Produit {brand}",
                'categories': categories,
                'tags': tags,
                'active': True,
                'source': 'real_scraping',
                'created_at': firestore.SERVER_TIMESTAMP,
                'popularity': 0,
                # Champs compatibles avec ancien schema
                'product_photo': product_data.get('image') or '',
                'product_title': product_data['name'],
                'product_url': url,
                'product_price': str(product_data.get('price') or 0),
            }

            # Logger les donn√©es
            log.write(f"    ‚úÖ Nom: {product['name']}\n")
            log.write(f"    üí∞ Prix: {product['price']}‚Ç¨\n")
            log.write(f"    üñºÔ∏è Image: {product['image'][:80]}...\n")
            log.write(f"    üè∑Ô∏è Tags: {', '.join(tags)}\n")
            log.write(f"    üìÇ Cat√©gories: {', '.join(categories)}\n")

            # Upload dans Firebase
            if upload_to_firebase(db, product):
                success_count += 1
                log.write(f"    ‚úÖ UPLOAD√â DANS FIREBASE\n")
            else:
                fail_count += 1
                log.write(f"    ‚ùå √âCHEC UPLOAD FIREBASE\n")

            # D√©lai anti-blocage al√©atoire
            delay = random.uniform(2, 5)
            print(f"    ‚è≥ Pause {delay:.1f}s...")
            time.sleep(delay)

        # R√©sum√© final
        print("\n")
        print("=" * 60)
        print("üìä R√âSULTATS FINAUX:")
        print(f"   ‚úÖ {success_count} produits scrap√©s et upload√©s avec succ√®s")
        print(f"   ‚ùå {fail_count} √©checs")
        print("=" * 60)

        log.write(f"\n\n{'='*60}\n")
        log.write(f"R√âSUM√â:\n")
        log.write(f"   ‚úÖ Succ√®s: {success_count}\n")
        log.write(f"   ‚ùå √âchecs: {fail_count}\n")
        log.write(f"{'='*60}\n")

    # Fermer le navigateur
    driver.quit()
    print("\nüéâ SCRAPING TERMIN√â!")
    print(f"üìù Logs sauvegard√©s dans: {LOG_FILE}")

# ============================================
# POINT D'ENTR√âE
# ============================================

if __name__ == "__main__":
    scrape_and_upload()
